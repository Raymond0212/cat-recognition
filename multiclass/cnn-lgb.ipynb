{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.optimizers as opt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, MaxPooling1D, BatchNormalization, Activation, Layer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.initializers import  RandomNormal, Constant\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, plot_metric\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "all kinds of cats:  ['bljfm', 'bom', 'dlm', 'fksm', 'jjc', 'jm', 'lm', 'md', 'mgbwm', 'mmm', 'mym', 'nnm', 'xmly', 'yjc', 'zem']\nbljfm :  258\nbom :  257\ndlm :  222\nfksm :  222\njjc :  246\njm :  232\nlm :  270\nmd :  258\nmgbwm :  235\nmmm :  212\nmym :  222\nnnm :  200\nxmly :  204\nyjc :  234\nzem :  235\ntotal cat imgs:  3507\n"
    }
   ],
   "source": [
    "path = \"./img_sub/cat\"\n",
    "dirlist = os.listdir(path)\n",
    "catPath = [os.path.join(path, dirname) for dirname in dirlist]\n",
    "\n",
    "total_img_num = sum(map(len,map(os.listdir, catPath)))\n",
    "print('all kinds of cats: ', dirlist)\n",
    "for i in range(len(catPath)):\n",
    "    print(dirlist[i], ': ', len(os.listdir(catPath[i])))\n",
    "print('total cat imgs: ', total_img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 5\n",
    "IMG_HEIGHT = 116\n",
    "IMG_WIDTH = 116\n",
    "split = 0.2\n",
    "classNum = len(dirlist)\n",
    "total_val = total_img_num * split\n",
    "total_train = total_img_num * (1 - split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,rotation_range=40, # Angle, 0-180\n",
    "    width_shift_range=0.2, # horizontal shifting\n",
    "    height_shift_range=0.2, # vertical shifting\n",
    "    shear_range=0.2, # Shearing\n",
    "    zoom_range=0.2, # Zooming\n",
    "    horizontal_flip=True, # Flipping\n",
    "    validation_split=split\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,rotation_range=40, # Angle, 0-180\n",
    "    width_shift_range=0.2, # horizontal shifting\n",
    "    height_shift_range=0.2, # vertical shifting\n",
    "    shear_range=0.2, # Shearing\n",
    "    zoom_range=0.2, # Zooming\n",
    "    horizontal_flip=True, # Flipping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 2811 images belonging to 15 classes.\nFound 696 images belonging to 15 classes.\nTrain set shape:  (2811,)\nValidation/Test set shape:  (696,)\n"
    }
   ],
   "source": [
    "# Load images from the disk, applies rescaling, and resizes the images\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=path,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ") # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    directory=path,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ") # set as validation data\n",
    "\n",
    "print(\"Train set shape: \", train_generator.classes.shape)\n",
    "print(\"Validation/Test set shape: \", validation_generator.classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From C:\\Users\\cs623\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From C:\\Users\\cs623\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From C:\\Users\\cs623\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"
    }
   ],
   "source": [
    "model = load_model('./multiclass/f_3355_60.180.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/5\n280/280 [==============================] - 277s 988ms/step - loss: 2.5807 - acc: 0.1627 - val_loss: 2.3202 - val_acc: 0.2371\nEpoch 2/5\n280/280 [==============================] - 311s 1s/step - loss: 2.2438 - acc: 0.2700 - val_loss: 2.1550 - val_acc: 0.2888\nEpoch 3/5\n165/280 [================>.............] - ETA: 1:54 - loss: 2.1117 - acc: 0.3254"
    }
   ],
   "source": [
    "a.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch = total_train // batch_size,\n",
    "        epochs = epochs,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = total_val // batch_size     \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1_layer_model = Model(inputs=a.input,\n",
    "                           outputs=a.get_layer('act5').output)\n",
    "\n",
    "dense1_output = dense1_layer_model.predict(train_generator)\n",
    "dense1_val = dense1_layer_model.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dense1_output\n",
    "y_train = train_generator.classes\n",
    "x_test = dense1_val\n",
    "y_test = validation_generator.classes\n",
    "\n",
    "print(x_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = xgb.XGBClassifier()\n",
    "xb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb.score(x_train,y_train))\n",
    "print(xb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\" : \"softmax\",\n",
    "          \"num_class\" : 15,\n",
    "          \"num_leaves\" : 50,\n",
    "          \"max_depth\": -1,\n",
    "          \"learning_rate\" : 0.01,\n",
    "          \"bagging_fraction\" : 0.9,  # subsample\n",
    "          \"feature_fraction\" : 0.9,  # colsample_bytree\n",
    "          \"bagging_freq\" : 5,        # subsample_freq\n",
    "          \"bagging_seed\" : 2018,\n",
    "          \"verbosity\" : -1,\n",
    "          \"metric\" : \"softmax\"\n",
    "}\n",
    "\n",
    "classifier_params = {\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'num_leaves' : 31,\n",
    "    'max_depth' : -1,\n",
    "    'learning_rate' : 0.1,\n",
    "    'n_estimators' : 100,\n",
    "    'subsample_for_bin' : 200000,\n",
    "    'objective' : 'softmax',\n",
    "    'class_weight' : None,\n",
    "    'min_split_gain' : 0.0,\n",
    "    'min_child_weight' : 0.001,\n",
    "    'min_child_samples' : 20,\n",
    "    'subsample' : 1.0,\n",
    "    'subsample_freq' : 0,\n",
    "    'colsample_bytree' : 1.0,\n",
    "    'reg_alpha' : 0.0,\n",
    "    'reg_lambda' : 0.0,\n",
    "    'random_state' : '10927',\n",
    "    'n_jobs' : -1,\n",
    "    'silent' : True,\n",
    "    'importance_type' : 'split'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lgb.Dataset(x_train, label = y_train)\n",
    "dataset.construct()\n",
    "vali = dataset.create_valid(x_test, label = y_test)\n",
    "vali.construct()\n",
    "print(dataset.num_data())\n",
    "print(vali.num_data())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_hist = lgb.cv(params, train_set = dataset)\n",
    "# trained_model = lgb.train(params, train_set = dataset)\n",
    "trained_model = lgb.train(params, train_set = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=20,\n",
    "    max_depth=50,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=30,\n",
    "    subsample_for_bin=10,\n",
    "    objective='softmax',\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,\n",
    "    min_child_weight=0.001,\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,\n",
    "    subsample_freq=0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state='10927',\n",
    "    n_jobs=-1,\n",
    "    silent=True,\n",
    "    importance_type='split'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = 0\n",
    "for i in range(len(result)):\n",
    "    if result[i] == y_test[i]:\n",
    "        corr += 1\n",
    "print(corr*1.0/len(result))\n",
    "print(y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}